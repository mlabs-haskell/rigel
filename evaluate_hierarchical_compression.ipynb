{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RANK=0\n",
      "env: WORLD_SIZE=1\n",
      "env: MASTER_ADDR=127.0.0.1\n",
      "env: MASTER_PORT=2020\n"
     ]
    }
   ],
   "source": [
    "%env RANK=0\n",
    "%env WORLD_SIZE=1\n",
    "%env MASTER_ADDR=127.0.0.1\n",
    "%env MASTER_PORT=2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_checkpoint_dir = \"modified_llama/llama-2-7b\"\n",
    "tokenizer_path = \"modified_llama/tokenizer.model\"\n",
    "compression_checkpoint_dir = \"cv_library/attention_model.pt\"\n",
    "max_seq_len = 128\n",
    "max_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_string = \"Neptune is the eighth and farthest known planet from the Sun.\"\n",
    "query_string = \"Neptune is the Roman god of freshwater and the sea in Roman religion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building generator...\n",
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 11.72 seconds\n",
      "Built generator!\n"
     ]
    }
   ],
   "source": [
    "from modified_llama.llama import Llama\n",
    "import torch\n",
    "\n",
    "# Create the Llama generator\n",
    "print(\"Building generator...\")\n",
    "generator = Llama.build(\n",
    "    ckpt_dir=llama_checkpoint_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    ")\n",
    "print(\"Built generator!\")\n",
    "\n",
    "# Tokenize the content and query, and generate context vectors for them\n",
    "content_tokens = generator.tokenize(max_seq_len, [(\"\", content_string)])\n",
    "content_tokens = [l for _, l in content_tokens]\n",
    "query_tokens = generator.tokenize(max_seq_len, [(\"\", query_string)])\n",
    "query_tokens = [l for _, l in query_tokens]\n",
    "\n",
    "content_cvs = generator.generate(content_tokens, max_gen_len=len(content_tokens[0]))\n",
    "query_cvs = generator.generate(query_tokens, max_gen_len=len(query_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer #1 output size: 512\n",
      "Layer #2 output size: 64\n",
      "Layer #3 output size: 8\n",
      "Layer #1 output size: 512\n",
      "Layer #2 output size: 64\n",
      "Layer #3 output size: 8\n",
      "Neptune is the eighth and farthest known planet from the Sun.\n",
      "Neptune is the Roman god of freshwater and the sea in Roman religion.\n",
      "512: 0.8026992082595825\n",
      "64: 0.34545978903770447\n",
      "8: 0.5567335486412048\n"
     ]
    }
   ],
   "source": [
    "from cv_library.hierarchical_compression import HierarchicalAttention\n",
    "from cv_library.loss_functions import sequence_similarity\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the compression network\n",
    "torch.set_default_dtype(torch.float32)\n",
    "with torch.device(\"cuda\"):\n",
    "    compression_network = HierarchicalAttention(content_cvs.shape)\n",
    "    checkpoint_path = Path(compression_checkpoint_dir)\n",
    "    if checkpoint_path.is_file():\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        compression_network.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        print(\"ERROR: Path provided for hierarchical compression model not a file\")\n",
    "        exit(1)\n",
    "    compressed_content = compression_network.forward(content_cvs.clone().to(torch.float32))\n",
    "\n",
    "    compression_network = HierarchicalAttention(query_cvs.shape)\n",
    "    checkpoint_path = Path(compression_checkpoint_dir)\n",
    "    if checkpoint_path.is_file():\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        compression_network.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        print(\"ERROR: Path provided for hierarchical compression model not a file\")\n",
    "        exit(1)\n",
    "    compressed_query = compression_network.forward(query_cvs.clone().to(torch.float32))\n",
    "\n",
    "    print(content_string)\n",
    "    print(query_string)\n",
    "    for cc, cq in zip(compressed_content, compressed_query):\n",
    "        vector_size = cc.shape[-1]\n",
    "        sim_score = sequence_similarity(cc, cq)\n",
    "        #sim_score = torch.dot(F.normalize(cc.squeeze(), dim=-1), F.normalize(cq.squeeze(), dim=-1))\n",
    "        print(f\"{vector_size}: {sim_score.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
